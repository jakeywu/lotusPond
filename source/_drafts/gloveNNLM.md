---
title: gloveNNLM
tags: undefined
categories: preview
date: 2017-11-13 13:59:05
---

语义嵌入和神经网络语言, 语音自动识别模型
NNLM(Neural Network Language Model): 神经网络语言模型
语义嵌入算法（例如:Word2vec和GloVe）的目标是 从文本中获取语义的信息
语义嵌入相比字嵌入多样化, 词嵌入是根据神经网络语言模型NNLM
SimRank 是一种基于图的拓扑结构信息来衡量任意两个对象间相似程度的模型, 核心思想为: 如果两个对象和被其相似的对象所引用(即它们有相似的入邻边结构), 那么这两个对象也相似
采用余弦相似度的GloVe和前馈NNLM(fnnlm)嵌入

GloVe performs a bilinear approximation of the word co-occurrence matrix computed over training data
                双线性         相似                     共现         矩阵
The V x V dimensional word co-occurrence matrix C is obtained by traversing training text and counting co-occurrences
            维度                                         获得        遍历
LM(language model) training 语言模型训练

word2vec  一款将词表征为实数值向量的高效工具. 一种典型的神经网络语言模型. 预测word o 出现在word c的上下文语境里的条件概率. 模型的损失函数是一个对数似然函数.              

MLE  极大似然估计法
概率: 已知参数, 求各种情况的可能性
似然: 已知结果, 推测参数

二项分布概率:
二项分布用符号b(x．n．p)，表示在n次试验中有x次成功，成功的概率为p。
　　b(x．n．p)=C_n^xp^xq^{n-x}

　　式中x＝0、1、2、3．．．．．n为正整数

　　C_n^x=\frac{n!}{x!(n-x)!}

向量空间模型VSM: (Vector space model)将词语表示为一个连续的词向量, 且语义接近的单词对应的词向量在空间上(欧式距离, cosine相似度等)也是接近的.
分布假说: 若两个单词的上下文相同则两个单词的语义也相同.
基于分布假说的词向量的方法分两类:
    统计法(count-based methods), 比如潜在语义分析(Latent Semantic Analysis, LSA)
    预测法(predictive methods), 比如神经网络语言模型(Neural Network Language Model)
 
N-gram模型(N元组模型): 传统的统计词向量模型使用单词在特定上下文中出现的概率表征这个句子是自然语言的概率 p(sentence) = p(word|context), 劣势:计算量过大, 计算一次条件概率就要扫描一遍语料库

连续词袋模型CBOW(Continuous Bag-of-Word Model):  三层神经网络, 输入上下文, 输出对下个单词的预测.
    第一层: 输入已知上下文的词向量
    中间层: 线性隐含层, 它将所有输入的词向量累加
    第三层: 一棵哈夫曼树, 树的的叶节点与语料库中的单词一一对应, 而树的每个非叶节点是一个二分类器, 直接与隐含层连接
步骤: 
    根据预料库建立词汇表, 词汇表中所有单词拥有一个随机的词向量.我们从语料库选择一段文本进行训练.
    将单词W的上下文的词向量输入CBOW, 由隐含层累加, 在第三层的哈夫曼树中沿着某个特定的路径到达某个叶节点, 从给出对单词W的预测.
    采用梯度下降法调整输入的词向量
    
    梯度下降法: 最速下降法(teepest descend method)： 梯度的方向与取得最大方向导数值的方向一致，而梯度的模就是函数在该点的方向导数的最大值    
Skip-gram模型:  三层神经网络. 输入一个单词, 输出对上下文的预测. 核心: 每一个单词从树根开始到达叶节点可以预测出它上下文中的一个单词

哈夫曼树(最优二叉树): 最高效的判别数, 带权路径长度WPL最小的二叉树称为赫夫曼树或最优二叉树    
树的基本概念和术语:
路径: 若树中存在一个结点序列k1,k2,…,kj, 使得ki是ki+1的双亲, 则称该结点序列是从k1到kj的一条路径.
路径长度: 等于路径上的结点数减1.
结点的权: 将树中的结点赋予一个有意义的数, 称为该结点的权.
结点的带权路径长度(WPL): 是指该结点到树根之间的路径长度与该结点上权的乘积.
  
哈夫曼树的构造(哈夫曼算法)
1.根据给定的n个权值{w1,w2,…,wn}构成二叉树集合F={T1,T2,…,Tn},其中每棵二叉树Ti中只有一个带权为wi的根结点,其左右子树为空.
2.在F中选取两棵根结点权值最小的树作为左右子树构造一棵新的二叉树,且置新的二叉树的根结点的权值为左右子树根结点的权值之和.
3.在F中删除这两棵树,同时将新的二叉树加入F中.
4.重复2、3,直到F只含有一棵树为止.(得到哈夫曼树)

满而叉树: 除最后一层无任何子节点外，每一层上的所有结点都有两个子结点二叉树
哈夫曼树中权越大的叶子离根越近
具有相同带权结点的哈夫曼树不惟一
哈夫曼树的结点的度数为 0 或 2， 没有度为 1 的结点。
包含 n 个叶子结点的哈夫曼树中共有 2n – 1 个结点。
包含 n 棵树的森林要经过 n–1 次合并才能形成哈夫曼树，共产生 n–1 个新结点

NN模型(neural network): 神经网络模型, 是复杂神经网络模型的基础计算单元. 激活函数主要有3种: sigmoid函数, tanh函数以及ReLU函数
FFNN模型(feedforward neural network): 前馈神经网络模型, 在输入层输入数据以后, 根据NN模型一次计算得到输出, 优化了损失函数
RNN模型(recurrent neural network): 递归神经网络模型, 具有反馈结构的神经网络模型,  其输出不但与当前输入和网络的权值有关，而且也与之前网络的输入有关. BPTT算法回传数据
LSTM模型(long-short term memory): 长短期记忆模型, 特殊的RNN模型, 采用存储单元来存储记忆. 

GloVe模型: 
模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息.
输入：语料库
输出：词向量
方法概述：首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量.

word2vec 没有考虑词序, 没有考虑全局统计信息. 可以很快提供一个不错的word embedding层的初始值.
GloVe  将全局词-词共现矩阵进行分些, 比词-文档矩阵更加稠密. 对低频词和高频词做了弱化处理. 

1. 构建词-词共现矩阵, 共现是建立在一个给定范围内, 得到一个V*V的矩阵. V是词汇表的大小. 因为不同频次的词对目标贡献不同, 所以设定权重函数f(x). 模型函数最小得到最终的词向量. 

全局对数双线性回归 == 全局矩阵分解 + 本地上下文窗口方法
global log-bilinear regression model == global matrix factorization + local context window methods
glove通过训练词-词共现矩阵的非零协作元素, 有效利用了统计信息  而非简单地稀疏矩阵或者大语料库中的单个上下文

CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别